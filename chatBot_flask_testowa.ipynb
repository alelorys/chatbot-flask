{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatBot flask testowa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1N6mCFZzNX1LEKMNbGkSLkzHFcgk7LzG5",
      "authorship_tag": "ABX9TyMeb1RR7ilnlk2FxDTEcP6S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alelorys/chatbot-flask/blob/main/chatBot_flask_testowa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SidMxxvxBvi4",
        "outputId": "20e2abed-333a-4d03-d26c-922f7c7dd466"
      },
      "source": [
        "!git clone https://github.com/alelorys/chatbot-flask"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatbot-flask'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 39 (delta 13), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (39/39), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oiii-6WPy2iB",
        "outputId": "c9b1cc4b-d4bd-4f58-ab8f-0d0bef9e863c"
      },
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install pystempel\n",
        "\n",
        "!pip install tflearn\n",
        "!pip install pyspellchecker "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting pystempel\n",
            "  Downloading pystempel-1.2.0-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pystempel) (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pystempel) (4.62.3)\n",
            "Installing collected packages: pystempel\n",
            "Successfully installed pystempel-1.2.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 50.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=619d87633fb081febd13868c23e9c26cd043e88d8a33e3d69a0b92542e4930d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.2-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxs0tQK6iL8n",
        "outputId": "157e1d3f-d180-4e74-f60d-9c28dbce4107"
      },
      "source": [
        "from flask import Flask, render_template, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import difflib\n",
        "\n",
        "import nltk\n",
        "import numpy\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.models import model_from_yaml\n",
        "from stempel import StempelStemmer\n",
        "\n",
        "#/content/drive/MyDrive/Colabooks/chatBoTest/static\n",
        "#/content/drive/MyDrive/Colabooks/chatBoTest/templates\n",
        "TEMPLATE = '/content/chatbot-flask/templates'\n",
        "STATIC = '/content/chatbot-flask/static'\n",
        "nltk.download('punkt')\n",
        "\n",
        "app = Flask(__name__, template_folder=TEMPLATE, static_folder=STATIC)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "stemmer = StempelStemmer.polimorf()\n",
        "\n",
        "with open(\"/content/chatbot-flask/q&a.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "try:\n",
        "    with open(\"/content/chatbot-flask/chatbot.pickle\", \"rb\") as file:\n",
        "        words, labels, training, output = pickle.load(file)\n",
        "\n",
        "except:\n",
        "    words = []\n",
        "    labels = []\n",
        "    docs_x = []\n",
        "    docs_y = []\n",
        "\n",
        "    for intent in data[\"questionAnswer\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            wrds = pattern.split(\" \")\n",
        "            words.extend(wrds)\n",
        "            docs_x.append(wrds)\n",
        "            docs_y.append(intent[\"tag\"])\n",
        "\n",
        "        if intent[\"tag\"] not in labels:\n",
        "            labels.append(intent[\"tag\"])\n",
        "\n",
        "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "    words = sorted(list(set(words)))\n",
        "\n",
        "    labels = sorted(labels)\n",
        "\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    output_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "    for x, doc in enumerate(docs_x):\n",
        "        bag = []\n",
        "\n",
        "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "\n",
        "        for w in words:\n",
        "            if w in wrds:\n",
        "                bag.append(1)\n",
        "            else:\n",
        "                bag.append(0)\n",
        "\n",
        "        output_row = output_empty[:]\n",
        "        output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "        training.append(bag)\n",
        "        output.append(output_row)\n",
        "\n",
        "    training = numpy.array(training)\n",
        "    output = numpy.array(output)\n",
        "\n",
        "    with open(\"/content/chatbot-flask/chatbot.pickle\", \"wb\") as file:\n",
        "        pickle.dump((words, labels, training, output), file)\n",
        "\n",
        "try:\n",
        "    yaml_file = open('/content/chatbot-flask/chatbotmodel.yaml', 'r')\n",
        "    loaded_model_yaml = yaml_file.read()\n",
        "    yaml_file.close()\n",
        "    myChatModel = model_from_yaml(loaded_model_yaml)\n",
        "    myChatModel.load_weights(\"/content/chatbot-flask/chatbotmodel.h5\")\n",
        "    \n",
        "\n",
        "except:\n",
        "    # Make our neural network\n",
        "    myChatModel = Sequential()\n",
        "    myChatModel.add(Dense(16, input_shape=[len(words)], activation='relu'))\n",
        "    myChatModel.add(Dense(16, activation='relu'))\n",
        "    myChatModel.add(Dense(len(labels), activation='softmax'))\n",
        "\n",
        "    # optimize the model\n",
        "    myChatModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # train the model\n",
        "    myChatModel.fit(training, output, epochs=160, batch_size=8, shuffle=True)\n",
        "\n",
        "    # serialize model to json and save it to disk\n",
        "    model_json = myChatModel.to_json()\n",
        "    with open(\"/content/chatbot-flask/chatbotmodel.json\", \"w\") as j_file:\n",
        "        j_file.write(model_json)\n",
        "\n",
        "    # serialize weights to HDF5\n",
        "    myChatModel.save_weights(\"/content/chatbot-flask/chatbotmodel.h5\")\n",
        "    print(\"Saved model from disk\")\n",
        "def bag_of_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "\n",
        "    s_words = s.split(\" \")\n",
        "\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "    for se in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "          seq = difflib.SequenceMatcher(None,w,se)\n",
        "          d = seq.ratio()*100\n",
        "          if w == se:\n",
        "            bag[i] = 1\n",
        "          if d >= 75:\n",
        "            bag[i] = 1\n",
        "          \n",
        "         \n",
        "            \n",
        "    return numpy.array(bag)\n",
        "\n",
        "def chatWithBot(userMess):\n",
        "    currentText = bag_of_words(userMess, words)\n",
        "    currentTextArray = [currentText]\n",
        "    numpyCurrentText = numpy.array(currentTextArray)\n",
        "    if numpy.all((numpyCurrentText == 0)):\n",
        "      answer = \"Nie rozumiem, zadaj pytanie jeszcze raz!\"\n",
        "      return answer\n",
        "\n",
        "    result = myChatModel.predict(numpyCurrentText[0:1])\n",
        "    result_index = numpy.argmax(result)\n",
        "    tag = labels[result_index]\n",
        "    if result[0][result_index] > 0.6:\n",
        "        for tg in data[\"questionAnswer\"]:\n",
        "            if tg['tag'] == tag:\n",
        "                responses = tg['responses']\n",
        "\n",
        "        answer = random.choice(responses)\n",
        "        return answer\n",
        "\n",
        "    else:\n",
        "        answer = \"Nie rozumiem, zadaj pytanie jeszcze raz!\"\n",
        "        return answer\n",
        "\n",
        "def chat(userMess):\n",
        "    \n",
        "\n",
        "    while True:\n",
        "        inp = userMess\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "        if inp.lower() == \"\" or inp.lower() == \" \":\n",
        "          answer = \"Musisz coś napisać!\"\n",
        "          return answer\n",
        "        \n",
        "        return chatWithBot(inp)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "    userMess = request.args.get(\"msg\")\n",
        "    return chat(userMess)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading: 100%|██████████| 11368252/11368252 [00:14<00:00, 807366.36bytes/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160\n",
            "26/26 [==============================] - 1s 2ms/step - loss: 4.4442 - accuracy: 0.0149 \n",
            "Epoch 2/160\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 4.4321 - accuracy: 0.0446\n",
            "Epoch 3/160\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 4.4213 - accuracy: 0.0644\n",
            "Epoch 4/160\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 4.4076 - accuracy: 0.0693\n",
            "Epoch 5/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 4.3891 - accuracy: 0.0891\n",
            "Epoch 6/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 4.3647 - accuracy: 0.1040\n",
            "Epoch 7/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 4.3328 - accuracy: 0.1089\n",
            "Epoch 8/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 4.2895 - accuracy: 0.1238\n",
            "Epoch 9/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 4.2370 - accuracy: 0.1238\n",
            "Epoch 10/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 4.1720 - accuracy: 0.1485\n",
            "Epoch 11/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 4.0971 - accuracy: 0.1485\n",
            "Epoch 12/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 4.0139 - accuracy: 0.1683\n",
            "Epoch 13/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 3.9224 - accuracy: 0.1980\n",
            "Epoch 14/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 3.8258 - accuracy: 0.2079\n",
            "Epoch 15/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 3.7238 - accuracy: 0.2079\n",
            "Epoch 16/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 3.6192 - accuracy: 0.2228\n",
            "Epoch 17/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 3.5118 - accuracy: 0.2426\n",
            "Epoch 18/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 3.4063 - accuracy: 0.2574\n",
            "Epoch 19/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 3.2960 - accuracy: 0.2624\n",
            "Epoch 20/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 3.1862 - accuracy: 0.2772\n",
            "Epoch 21/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 3.0778 - accuracy: 0.2921\n",
            "Epoch 22/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 2.9686 - accuracy: 0.2970\n",
            "Epoch 23/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 2.8602 - accuracy: 0.3317\n",
            "Epoch 24/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 2.7525 - accuracy: 0.3762\n",
            "Epoch 25/160\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 2.6453 - accuracy: 0.3911\n",
            "Epoch 26/160\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 2.5396 - accuracy: 0.4356\n",
            "Epoch 27/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 2.4312 - accuracy: 0.4505\n",
            "Epoch 28/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 2.3333 - accuracy: 0.4901\n",
            "Epoch 29/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 2.2301 - accuracy: 0.5198\n",
            "Epoch 30/160\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 2.1334 - accuracy: 0.5545\n",
            "Epoch 31/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 2.0395 - accuracy: 0.5792\n",
            "Epoch 32/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.9479 - accuracy: 0.5990\n",
            "Epoch 33/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.8628 - accuracy: 0.6535\n",
            "Epoch 34/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.7779 - accuracy: 0.6584\n",
            "Epoch 35/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.7001 - accuracy: 0.6881\n",
            "Epoch 36/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.6224 - accuracy: 0.7178\n",
            "Epoch 37/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.5499 - accuracy: 0.7376\n",
            "Epoch 38/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.4815 - accuracy: 0.7673\n",
            "Epoch 39/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.4130 - accuracy: 0.8020\n",
            "Epoch 40/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.3482 - accuracy: 0.8168\n",
            "Epoch 41/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2833 - accuracy: 0.8218\n",
            "Epoch 42/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2276 - accuracy: 0.8416\n",
            "Epoch 43/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.1723 - accuracy: 0.8366\n",
            "Epoch 44/160\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 1.1195 - accuracy: 0.8564\n",
            "Epoch 45/160\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 1.0701 - accuracy: 0.8713\n",
            "Epoch 46/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.0219 - accuracy: 0.8812\n",
            "Epoch 47/160\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.9761 - accuracy: 0.9059\n",
            "Epoch 48/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9386 - accuracy: 0.9059\n",
            "Epoch 49/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8913 - accuracy: 0.9158\n",
            "Epoch 50/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8534 - accuracy: 0.9406\n",
            "Epoch 51/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8140 - accuracy: 0.9257\n",
            "Epoch 52/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7821 - accuracy: 0.9455\n",
            "Epoch 53/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7477 - accuracy: 0.9455\n",
            "Epoch 54/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7181 - accuracy: 0.9455\n",
            "Epoch 55/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.9455\n",
            "Epoch 56/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.9554\n",
            "Epoch 57/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6331 - accuracy: 0.9554\n",
            "Epoch 58/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6082 - accuracy: 0.9604\n",
            "Epoch 59/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.9505\n",
            "Epoch 60/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5602 - accuracy: 0.9604\n",
            "Epoch 61/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.9653\n",
            "Epoch 62/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.9703\n",
            "Epoch 63/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.9802\n",
            "Epoch 64/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.9653\n",
            "Epoch 65/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.9703\n",
            "Epoch 66/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.9703\n",
            "Epoch 67/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.9851\n",
            "Epoch 68/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.9802\n",
            "Epoch 69/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.9851\n",
            "Epoch 70/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3780 - accuracy: 0.9901\n",
            "Epoch 71/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3629 - accuracy: 0.9851\n",
            "Epoch 72/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3484 - accuracy: 0.9851\n",
            "Epoch 73/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3381 - accuracy: 0.9851\n",
            "Epoch 74/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.9901\n",
            "Epoch 75/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3137 - accuracy: 0.9802\n",
            "Epoch 76/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3039 - accuracy: 0.9802\n",
            "Epoch 77/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.9851\n",
            "Epoch 78/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2830 - accuracy: 0.9802\n",
            "Epoch 79/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.9802\n",
            "Epoch 80/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.9901\n",
            "Epoch 81/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.9851\n",
            "Epoch 82/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2460 - accuracy: 0.9851\n",
            "Epoch 83/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9851\n",
            "Epoch 84/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2308 - accuracy: 0.9851\n",
            "Epoch 85/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9901\n",
            "Epoch 86/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2154 - accuracy: 0.9851\n",
            "Epoch 87/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2093 - accuracy: 0.9901\n",
            "Epoch 88/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.9950\n",
            "Epoch 89/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1959 - accuracy: 0.9950\n",
            "Epoch 90/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1894 - accuracy: 1.0000\n",
            "Epoch 91/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1845 - accuracy: 0.9901\n",
            "Epoch 92/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1776 - accuracy: 0.9950\n",
            "Epoch 93/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1737 - accuracy: 0.9851\n",
            "Epoch 94/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1664 - accuracy: 1.0000\n",
            "Epoch 95/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1631 - accuracy: 0.9950\n",
            "Epoch 96/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1567 - accuracy: 1.0000\n",
            "Epoch 97/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1520 - accuracy: 1.0000\n",
            "Epoch 98/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1475 - accuracy: 1.0000\n",
            "Epoch 99/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1432 - accuracy: 1.0000\n",
            "Epoch 100/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1398 - accuracy: 1.0000\n",
            "Epoch 101/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1337 - accuracy: 1.0000\n",
            "Epoch 102/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1302 - accuracy: 1.0000\n",
            "Epoch 103/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1267 - accuracy: 1.0000\n",
            "Epoch 104/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 1.0000\n",
            "Epoch 105/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1190 - accuracy: 1.0000\n",
            "Epoch 106/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1160 - accuracy: 1.0000\n",
            "Epoch 107/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1114 - accuracy: 1.0000\n",
            "Epoch 108/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1095 - accuracy: 1.0000\n",
            "Epoch 109/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1050 - accuracy: 1.0000\n",
            "Epoch 110/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1015 - accuracy: 1.0000\n",
            "Epoch 111/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0991 - accuracy: 1.0000\n",
            "Epoch 112/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 1.0000\n",
            "Epoch 113/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 1.0000\n",
            "Epoch 114/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 1.0000\n",
            "Epoch 115/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 1.0000\n",
            "Epoch 116/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 1.0000\n",
            "Epoch 117/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0825 - accuracy: 1.0000\n",
            "Epoch 118/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 1.0000\n",
            "Epoch 119/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 1.0000\n",
            "Epoch 120/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0754 - accuracy: 1.0000\n",
            "Epoch 121/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0737 - accuracy: 1.0000\n",
            "Epoch 122/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 1.0000\n",
            "Epoch 123/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0688 - accuracy: 1.0000\n",
            "Epoch 124/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0678 - accuracy: 1.0000\n",
            "Epoch 125/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 1.0000\n",
            "Epoch 126/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 1.0000\n",
            "Epoch 127/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0615 - accuracy: 1.0000\n",
            "Epoch 128/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 1.0000\n",
            "Epoch 129/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0582 - accuracy: 1.0000\n",
            "Epoch 130/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 1.0000\n",
            "Epoch 131/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 1.0000\n",
            "Epoch 132/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 1.0000\n",
            "Epoch 133/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 1.0000\n",
            "Epoch 134/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 1.0000\n",
            "Epoch 135/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0487 - accuracy: 1.0000\n",
            "Epoch 136/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 1.0000\n",
            "Epoch 137/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 1.0000\n",
            "Epoch 138/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0446 - accuracy: 1.0000\n",
            "Epoch 139/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0436 - accuracy: 1.0000\n",
            "Epoch 140/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0425 - accuracy: 1.0000\n",
            "Epoch 141/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0414 - accuracy: 1.0000\n",
            "Epoch 142/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 1.0000\n",
            "Epoch 143/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 1.0000\n",
            "Epoch 144/160\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 145/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 146/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 147/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 148/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 1.0000\n",
            "Epoch 149/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 1.0000\n",
            "Epoch 150/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 1.0000\n",
            "Epoch 151/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 1.0000\n",
            "Epoch 152/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 1.0000\n",
            "Epoch 153/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 1.0000\n",
            "Epoch 154/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 1.0000\n",
            "Epoch 155/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 1.0000\n",
            "Epoch 156/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 1.0000\n",
            "Epoch 157/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 158/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 1.0000\n",
            "Epoch 159/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 1.0000\n",
            "Epoch 160/160\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 1.0000\n",
            "Saved model from disk\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://196f-104-196-208-225.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [28/Oct/2021 08:01:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Oct/2021 08:01:08] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Oct/2021 08:01:08] \"\u001b[37mGET /static/chat.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Oct/2021 08:01:09] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [28/Oct/2021 08:01:15] \"\u001b[37mGET /get?msg=Hejo HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Oct/2021 08:01:22] \"\u001b[37mGET /get?msg=Hejka HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Oct/2021 08:01:26] \"\u001b[37mGET /get?msg=Hej HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Oct/2021 08:01:51] \"\u001b[37mGET /get?msg=Jak%20uczyć%20się%20programowania%3F HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Oct/2021 08:02:18] \"\u001b[37mGET /get?msg=Typy%20juniorów HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Oct/2021 08:02:36] \"\u001b[37mGET /get?msg=O%20co%20chodzi%20z%20home%20alone%3F HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        }
      ]
    }
  ]
}